# -*- coding: utf-8 -*-
"""MOFARO_Simulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13UNwfmJFfSCh2b7B7p-KmK3_a8YXzOfR
"""

import numpy as np
import networkx as nx
from scipy.stats import levy_stable
import random
import math
import copy
import time
import matplotlib.pyplot as plt

# Constants (optimized, edge only for MOFARO)
NUM_TASKS = 50
NUM_EDGE_NODES = 30  # For MOFARO
NUM_FOG_NODES = 5
NUM_CLOUD_NODES = 1
POP_SIZE = 5
MAX_IT = 5
LEVY_BETA = 1.5
P_LOSS_MIN = 0.05
P_LOSS_MAX = 0.1
BETA_ENERGY = 1e-12
P_TX = 0.1
C_DVFS = 1e-9
V_DVFS = 1.2
F_DVFS = 1.0
OVERHEAD_TIME = 0.1
OVERHEAD_COST = 0.01
W_QOS = 1.0
MARKOV_STATES = 2
TRANS_PROB = 0.8
ALPHA = 0.2
NRUN = 5
FTC_DELAY = random.randint(30, 300)
MAX_Q = 3
N_USERS = 100
FRACTAL_DIM_MIN = 1.5
FRACTAL_DIM_MAX = 2.0

# Generate parameters for MOFARO (with edge)
def generate_parameters():
    tasks = []
    for i in range(NUM_TASKS):
        s = random.randint(100, 500)
        dl = random.randint(100, 1000)
        task = {
            'id': i,
            's': s,
            'm': random.randint(50, 200),
            'd': dl,
            'p': random.uniform(0.01, 0.5),
            'q': random.uniform(1, MAX_Q),
            'in_': random.randint(100, 1000),
            'out': random.randint(1, 100),
            'resp': 0
        }
        tasks.append(task)
    nodes = []
    for j in range(NUM_EDGE_NODES + NUM_FOG_NODES + NUM_CLOUD_NODES):
        layer = 'edge' if j < NUM_EDGE_NODES else 'fog' if j < NUM_EDGE_NODES + NUM_FOG_NODES else 'cloud'
        c = random.randint(1000, 5000)
        m = random.randint(512, 4096)
        b = 500
        dis = random.randint(1, 3)
        pc = random.uniform(0.05, 0.2)
        d = dis * random.randint(1, 2)
        pmax = random.randint(50, 100)
        pmin = random.uniform(0.5, 0.6) * pmax
        node = {
            'id': j,
            'layer': layer,
            'c': c,
            'm': m,
            'b': b,
            'pc': pc,
            'd': d,
            'dis': dis,
            'pmax': pmax,
            'pmin': pmin,
            'a': 0,
            'ms': 0,
            'eng': 0,
            'rcl': 0,
            'available_c': c,
            'available_m': m,
            'engCons': 0,
            'resp': 0
        }
        nodes.append(node)
    bw_matrix = np.random.uniform(100, 500, (len(nodes), len(nodes)))
    lat_matrix = np.random.uniform(10, 100, (len(nodes), len(nodes)))
    np.fill_diagonal(bw_matrix, np.inf)
    np.fill_diagonal(lat_matrix, 0)
    dist_matrix = np.random.uniform(10, 100, (len(nodes), len(nodes)))
    np.fill_diagonal(dist_matrix, 0)
    G = nx.DiGraph()
    for i in range(NUM_TASKS):
        G.add_node(i)
    if NUM_TASKS > 1:
        G.add_edge(0, 1, weight=random.uniform(1, 10))
    transition_matrix = np.full((MARKOV_STATES, MARKOV_STATES), (1 - TRANS_PROB) / (MARKOV_STATES - 1))
    np.fill_diagonal(transition_matrix, TRANS_PROB)
    return tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix

# t_proc
def t_proc(task_s, node_c):
    base = (task_s / node_c) * 1000
    return base + OVERHEAD_TIME * base

# t_comm
def t_comm(task_d, j, k, bw_matrix, lat_matrix, nodes):
    if j == k:
        return 0
    p_loss = random.uniform(P_LOSS_MIN, P_LOSS_MAX)
    bw = bw_matrix[j, k]
    if bw == 0:
        bw = 1e-6
    delay = lat_matrix[j, k]
    if nodes[j]['layer'] == 'fog' and nodes[k]['layer'] == 'cloud':
        delay += FTC_DELAY
    return (task_d / bw) / (1 - p_loss) + delay

# calculate_exec_times
def calculate_exec_times(alloc, tasks, nodes, G, bw_matrix, lat_matrix):
    num_tasks = len(tasks)
    num_nodes = len(nodes)
    est = [0.0] * num_tasks
    exec_times = [0.0] * num_tasks
    complete_times = [0.0] * num_tasks
    node_finish = [0.0] * num_nodes
    topo_order = list(nx.topological_sort(G))
    for i in topo_order:
        j = np.argmax(alloc[i])
        ready_time = 0.0
        for pred in G.predecessors(i):
            k = np.argmax(alloc[pred])
            comm_sum = sum(t_comm(tasks[pred]['out'], k, j, bw_matrix, lat_matrix, nodes) for _ in range(N_USERS))
            avg_comm = comm_sum / N_USERS if k != j else 0.0
            ready_time = max(ready_time, complete_times[pred] + avg_comm)
        est[i] = max(ready_time, node_finish[j])
        exec_times[i] = t_proc(tasks[i]['s'], nodes[j]['c'])
        complete_times[i] = est[i] + exec_times[i]
        node_finish[j] = complete_times[i]
    lft = [math.inf] * num_tasks
    rev_topo = list(reversed(topo_order))
    for i in rev_topo:
        min_succ = tasks[i]['d']
        for succ in G.successors(i):
            k = np.argmax(alloc[succ])
            j = np.argmax(alloc[i])
            comm_sum = sum(t_comm(tasks[i]['out'], j, k, bw_matrix, lat_matrix, nodes) for _ in range(N_USERS))
            avg_comm = comm_sum / N_USERS if j != k else 0.0
            min_succ = min(min_succ, lft[succ] - exec_times[succ] - avg_comm)
        lft[i] = min_succ
    return est, lft, exec_times, complete_times

# mob_constraint
def mob_constraint(task, transition_matrix):
    pred_mob = 0
    for u in range(N_USERS):
        path_prob = 1.0
        current_state = random.randint(0, MARKOV_STATES-1)
        for _ in range(3):
            next_state = np.argmax(transition_matrix[current_state])
            path_prob *= transition_matrix[current_state, next_state]
            current_state = next_state
        cost_mob = random.uniform(0, 1)
        pred_mob += path_prob * cost_mob
    return pred_mob / N_USERS

# check_constraints
def check_constraints(alloc, tasks, nodes, G, complete_times):
    penalty = 0
    num_nodes = len(nodes)
    node_load_m = [0] * num_nodes
    for i in range(len(tasks)):
        j = np.argmax(alloc[i])
        node_load_m[j] += tasks[i]['m']
        if node_load_m[j] > nodes[j]['m']:
            penalty += 1000 * (node_load_m[j] - nodes[j]['m'])
        over = complete_times[i] - tasks[i]['d']
        if over > 0:
            penalty += 1500 * over * tasks[i]['p']
            if nodes[j]['layer'] == 'cloud':
                penalty += 500 * over
    return penalty

# z1
def z1(complete_times):
    return max(complete_times) if complete_times else 0

# z2
def z2(alloc, tasks, nodes, exec_times, dist_matrix, G, complete_times):
    energy = 0
    M = z1(complete_times)
    for i in range(len(tasks)):
        j = np.argmax(alloc[i])
        p_j = C_DVFS * V_DVFS**2 * F_DVFS
        e_proc = p_j * exec_times[i]
        energy += e_proc
        e_comm_sum = 0
        for succ in G.successors(i):
            k = np.argmax(alloc[succ])
            d_bits = tasks[i]['out'] * 8 * 1024
            for u in range(N_USERS):
                e_comm_sum += (BETA_ENERGY * d_bits + P_TX) * dist_matrix[j][k]
        energy += e_comm_sum / N_USERS
    node_a = [0.0] * len(nodes)
    for i in range(len(tasks)):
        j = np.argmax(alloc[i])
        node_a[j] += exec_times[i]
    for jj in range(len(nodes)):
        a = node_a[jj]
        energy += (a / 1000) * nodes[jj]['pmax'] + ((M / 1000) - (a / 1000)) * nodes[jj]['pmin']
    energy += 0.1 * energy
    return energy

# z3
def z3(alloc, tasks, nodes, exec_times):
    cost = 0
    for i in range(len(tasks)):
        j = np.argmax(alloc[i])
        c_proc = nodes[j]['pc'] * exec_times[i]
        c_net = 0.01 * tasks[i]['d']
        c_store = 0.001 * tasks[i]['d'] * exec_times[i]
        cost += c_proc + c_net + c_store + OVERHEAD_COST * c_proc
    return cost

# z4
def z4(tasks, complete_times):
    qos = 0
    max_q = MAX_Q
    for i in range(len(tasks)):
        qos += (1 - tasks[i]['q'] / max_q) * complete_times[i]
        qos += max(0, complete_times[i] - tasks[i]['d']) * W_QOS
    return qos

# evaluate_objs
def evaluate_objs(alloc, tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix, exec_times, complete_times):
    penalty = check_constraints(alloc, tasks, nodes, G, complete_times)
    z1_val = z1(complete_times) + penalty
    z2_val = z2(alloc, tasks, nodes, exec_times, dist_matrix, G, complete_times) + penalty
    z3_val = z3(alloc, tasks, nodes, exec_times) + penalty
    z4_val = z4(tasks, complete_times) + penalty
    return [z1_val, z2_val, z3_val, z4_val]

# dominates
def dominates(a, b):
    return all(a[k] <= b[k] for k in range(4)) and any(a[k] < b[k] for k in range(4))

# non_dominated_sort
def non_dominated_sort(objectives):
    front = []
    for i in range(len(objectives)):
        dominated = False
        for j in range(len(objectives)):
            if i != j and dominates(objectives[j], objectives[i]):
                dominated = True
                break
        if not dominated:
            front.append(i)
    return front

# crowding_distance
def crowding_distance(objectives):
    if len(objectives) < 2:
        return [math.inf] * len(objectives)
    dist = [0] * len(objectives)
    for m in range(4):
        sorted_idx = np.argsort([obj[m] for obj in objectives])
        min_val = objectives[sorted_idx[0]][m]
        max_val = objectives[sorted_idx[-1]][m]
        dist[sorted_idx[0]] = math.inf
        dist[sorted_idx[-1]] = math.inf
        for i in range(1, len(objectives)-1):
            dist[sorted_idx[i]] += (objectives[sorted_idx[i+1]][m] - objectives[sorted_idx[i-1]][m]) / (max_val - min_val + 1e-10)
    return dist

# topsis
def topsis(front_objs):
    m = len(front_objs)
    if m == 0:
        return [math.inf] * 4
    n = 4
    decision_matrix = np.array(front_objs)
    normalized = decision_matrix / np.sqrt(np.sum(decision_matrix**2, axis=0))
    weights = np.ones(n) / n
    weighted = normalized * weights
    ideal = np.min(weighted, axis=0)
    anti_ideal = np.max(weighted, axis=0)
    sep_pos = np.sqrt(np.sum((weighted - ideal)**2, axis=1))
    sep_neg = np.sqrt(np.sum((weighted - anti_ideal)**2, axis=1))
    closeness = sep_neg / (sep_pos + sep_neg + 1e-10)
    best_idx = np.argmax(closeness)
    return front_objs[best_idx]

# update_resources
def update_resources(best_alloc, tasks, nodes, exec_times):
    for i in range(len(tasks)):
        j = np.argmax(best_alloc[i])
        nodes[j]['available_m'] -= tasks[i]['m']
        nodes[j]['available_m'] = max(0, nodes[j]['available_m'])
        nodes[j]['a'] += exec_times[i]

# handle_failure
def handle_failure(task, nodes):
    available = [n for n in nodes if n['available_m'] > task['m']]
    if available:
        return random.choice(available)['id']
    return random.randint(0, len(nodes)-1)

# sort_tasks
def sort_tasks(tasks):
    return sorted(tasks, key=lambda t: t['d'])

# sort_nodes_eng
def sort_nodes_eng(nodes, rcl_only=False):
    filtered = [n for n in nodes if not rcl_only or n['rcl'] == 1]
    sorted_nodes = sorted(filtered, key=lambda n: n['engCons'] + (0 if n['layer'] in ['edge', 'fog'] else 10))
    return sorted_nodes

# mofaro
def mofaro(tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix):
    num_tasks = len(tasks)
    num_nodes = len(nodes)
    tasks = sort_tasks(tasks)
    population = []
    for _ in range(POP_SIZE):
        alloc = np.zeros((num_tasks, num_nodes))
        for i in range(num_tasks):
            j = random.randint(0, num_nodes-1)
            alloc[i, j] = 1
        population.append(alloc)
    for it in range(MAX_IT):
        objectives = []
        for p_idx, alloc in enumerate(population):
            est, lft, exec_times, complete_times = calculate_exec_times(alloc, tasks, nodes, G, bw_matrix, lat_matrix)
            objs = evaluate_objs(alloc, tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix, exec_times, complete_times)
            objectives.append(objs)
        front_indices = non_dominated_sort(objectives)
        front_objs = [objectives[idx] for idx in front_indices if idx < len(objectives)]
        if len(front_objs) == 0:
            front_objs = objectives
            front_indices = list(range(len(objectives)))
        crowd_dist = crowding_distance(front_objs)
        sorted_front = sorted(range(len(front_objs)), key=lambda k: crowd_dist[k], reverse=True)
        selected = min(POP_SIZE, len(front_indices))
        selected_indices = [front_indices[s] for s in sorted_front[:selected]]
        new_pop = []
        e = 2 * (1 - it / MAX_IT)
        for p_idx in range(POP_SIZE):
            alloc = population[p_idx]
            for i in range(num_tasks):
                nodes_copy = copy.deepcopy(nodes)
                for j in range(num_nodes):
                    temp_alloc = alloc.copy()
                    temp_alloc[i] = np.zeros(num_nodes)
                    temp_alloc[i, j] = 1
                    _, _, _, complete_temp = calculate_exec_times(temp_alloc, tasks, nodes_copy, G, bw_matrix, lat_matrix)
                    nodes_copy[j]['resp'] = complete_temp[i]
                    if complete_temp[i] <= tasks[i]['d']:
                        nodes_copy[j]['rcl'] = 1
                    else:
                        nodes_copy[j]['rcl'] = 0
                    M = max(complete_temp)
                    engCons = 0
                    for k in range(num_nodes):
                        node_ms = random.uniform(0, M)
                        node_eng = (node_ms / 1000) * nodes_copy[k]['pmax'] + ((M / 1000) - (node_ms / 1000)) * nodes_copy[k]['pmin']
                        engCons += node_eng
                    nodes_copy[j]['engCons'] = engCons
                rcl_nodes = [n for n in nodes_copy if n['rcl'] == 1]
                rcl_size = len(rcl_nodes)
                if rcl_size == 0:
                    j = handle_failure(tasks[i], nodes)
                else:
                    x = int(ALPHA * rcl_size)
                    if x < 1:
                        x = 1
                    sorted_rcl = sort_nodes_eng(rcl_nodes)
                    y = random.randint(0, x-1)
                    j = sorted_rcl[y]['id']
                alloc[i] = np.zeros(num_nodes)
                alloc[i, j] = 1
            if random.random() < e:
                # Fractal-based exploration
                fractal_dim = random.uniform(FRACTAL_DIM_MIN, FRACTAL_DIM_MAX)
                theta = random.uniform(0, 2 * math.pi)
                r = random.random()
                fractal_step = math.sin(theta * fractal_dim) / (r ** LEVY_BETA)
                levy_s = levy_stable.rvs(alpha=LEVY_BETA, beta=1.0, size=num_tasks * num_nodes).reshape((num_tasks, num_nodes))
                levy_s = np.clip(levy_s, -1, 1)
                fractal_s = fractal_step * levy_s
                best_alloc = population[random.choice(selected_indices)]
                new_alloc = alloc + fractal_s * (best_alloc - alloc)
            else:
                global_best = population[selected_indices[0]]
                new_alloc = alloc + random.random() * (global_best - alloc)
            for i in range(num_tasks):
                row = new_alloc[i]
                j = np.argmax(row)
                new_alloc[i] = np.zeros(num_nodes)
                new_alloc[i, j] = 1
            for i in range(num_tasks):
                mob = random.uniform(0, 1)
                pred_mob = mob_constraint(tasks[i], transition_matrix)
                if mob > pred_mob:
                    new_alloc[i] = np.zeros(num_nodes)
                    new_alloc[i, handle_failure(tasks[i], nodes)] = 1
            new_pop.append(new_alloc)
        population = new_pop
    objectives = []
    for alloc in population:
        est, lft, exec_times, complete_times = calculate_exec_times(alloc, tasks, nodes, G, bw_matrix, lat_matrix)
        objs = evaluate_objs(alloc, tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix, exec_times, complete_times)
        objectives.append(objs)
    front_indices = non_dominated_sort(objectives)
    front_objs = [objectives[idx] for idx in front_indices if idx < len(objectives)]
    if len(front_objs) == 0:
        front_objs = objectives
    best_objs = topsis(front_objs)
    best_front_idx = np.argmin([sum(o) for o in front_objs])
    best_alloc = population[front_indices[best_front_idx] if len(front_indices) > 0 else 0]
    _, _, exec_times, _ = calculate_exec_times(best_alloc, tasks, nodes, G, bw_matrix, lat_matrix)
    update_resources(best_alloc, tasks, nodes, exec_times)
    return best_objs, best_alloc

# Old models classes
class TaskOld:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

class NodeOld:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

# generate_for_old_models (no edge for old models)
def generate_for_old_models(ntask, nfog, ncloud):
    tasks = []
    for i in range(ntask):
        x = random.randint(0, 1)
        if x == 0:
            s = random.randint(100, 372)
            d = random.randint(500, 2500)
        else:
            s = random.randint(1028, 4280)
            d = random.randint(500, 2500)
        m = random.randint(50, 200)
        p = random.uniform(0.01, 0.5)
        q = random.uniform(90, 99.99)
        in_ = random.randint(100, 10000)
        out = random.randint(1, 991)
        tasks.append(TaskOld(id=i, s=s, m=m, d=d, p=p, q=q, in_=in_, out=out, resp=0))
    nodes = []
    for j in range(nfog):
        c = random.randint(2000, 6000)
        m = random.randint(150, 250)
        b = 1000
        dis = random.randint(1, 5)
        pc = random.uniform(0.1, 0.4)
        d = dis * random.randint(1, 3)
        pmax = random.randint(80, 200)
        pmin = random.uniform(0.6, 0.7) * pmax
        nodes.append(NodeOld(id=j, layer='fog', c=c, m=m, b=b, dis=dis, pc=pc, d=d, pmax=pmax, pmin=pmin, a=0, ms=0, eng=0, rcl=0, engCons=0, resp=0))
    x = random.randint(200, 500)
    for j in range(nfog, nfog + ncloud):
        c = random.randint(3000, 5000)
        m = random.randint(8192, 65536)
        b = random.randint(100, 10000)
        pc = random.uniform(0.7, 1.0)
        d = x
        pmax = random.randint(200, 400)
        pmin = 0.6 * pmax
        nodes.append(NodeOld(id=j, layer='cloud', c=c, m=m, b=b, dis=dis, pc=pc, d=d, pmax=pmax, pmin=pmin, a=0, ms=0, eng=0, rcl=0, engCons=0, resp=0))
    return tasks, nodes

# compute_metrics_old
def compute_metrics_old(T, nodes, ntask):
    all_nodes = nodes
    total_penalty = sum(max(0, T[i].resp - T[i].d) for i in range(ntask))
    num_sat = sum(1 for i in range(ntask) if T[i].resp <= T[i].d)
    M = max(node.a for node in all_nodes) if all_nodes else 0
    eng_cons = sum((node.a / 1000 * node.pmax) + ((M / 1000 - node.a / 1000) * node.pmin) for node in all_nodes)
    total_cost = sum(node.a / 1000 * node.pc for node in all_nodes)
    pdst = num_sat / ntask if ntask > 0 else 0
    return M, num_sat, pdst, eng_cons, total_penalty, total_cost

# fcfs_old
def fcfs_old(ntask, nfog, ncloud):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(NRUN):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        for i in range(ntask):
            x = random.randint(0, len(nodes)-1)
            etime = T[i].s / nodes[x].c * 1000
            T[i].resp = nodes[x].a + etime + nodes[x].d + nodes[x].dis * (T[i].in_ + T[i].out) / nodes[x].b
            nodes[x].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / NRUN, sum_num_sat / NRUN, sum_pdst / NRUN, sum_eng / NRUN, sum_cost / NRUN, sum_pen / 1000 / NRUN]

# edf_old
def edf_old(ntask, nfog, ncloud):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(NRUN):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        T = sorted(T, key=lambda t: t.d)
        for i in range(ntask):
            x = random.randint(0, len(nodes)-1)
            etime = T[i].s / nodes[x].c * 1000
            T[i].resp = nodes[x].a + etime + nodes[x].d + nodes[x].dis * (T[i].in_ + T[i].out) / nodes[x].b
            nodes[x].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / NRUN, sum_num_sat / NRUN, sum_pdst / NRUN, sum_eng / NRUN, sum_cost / NRUN, sum_pen / 1000 / NRUN]

# gfe_old
def gfe_old(ntask, nfog, ncloud):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(NRUN):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        for i in range(ntask):
            min_eng = float('inf')
            index = 0
            for j in range(len(nodes)):
                etime = T[i].s / nodes[j].c * 1000
                resp = nodes[j].a + etime + nodes[j].d + nodes[j].dis * (T[i].in_ + T[i].out) / nodes[j].b
                M_temp = max(node.a + (etime if k == j else 0) for k in range(len(nodes)))
                eng_temp = sum(((node.a + (etime if k == j else 0)) / 1000 * node.pmax + (M_temp / 1000 - (node.a + (etime if k == j else 0)) / 1000) * node.pmin) for k, node in enumerate(nodes))
                if eng_temp < min_eng:
                    min_eng = eng_temp
                    index = j
            etime = T[i].s / nodes[index].c * 1000
            T[i].resp = nodes[index].a + etime + nodes[index].d + nodes[index].dis * (T[i].in_ + T[i].out) / nodes[index].b
            nodes[index].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / NRUN, sum_num_sat / NRUN, sum_pdst / NRUN, sum_eng / NRUN, sum_cost / NRUN, sum_pen / 1000 / NRUN]

# detour_old
def detour_old(ntask, nfog, ncloud):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(NRUN):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        for i in range(ntask):
            min_resp = float('inf')
            index = 0
            for j in range(len(nodes)):
                etime = T[i].s / nodes[j].c * 1000
                resp = nodes[j].a + etime + nodes[j].d + nodes[j].dis * (T[i].in_ + T[i].out) / nodes[j].b
                if resp < min_resp:
                    min_resp = resp
                    index = j
            etime = T[i].s / nodes[index].c * 1000
            T[i].resp = min_resp
            nodes[index].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / NRUN, sum_num_sat / NRUN, sum_pdst / NRUN, sum_eng / NRUN, sum_cost / NRUN, sum_pen / 1000 / NRUN]

# semi_greedy_old
def semi_greedy_old(ntask, nfog, ncloud):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(NRUN):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        T = sorted(T, key=lambda t: t.d)
        for i in range(ntask):
            rcl = []
            for j in range(len(nodes)):
                etime = T[i].s / nodes[j].c * 1000
                resp = nodes[j].a + etime + nodes[j].d + nodes[j].dis * (T[i].in_ + T[i].out) / nodes[j].b
                M_temp = max(node.a + (etime if k == j else 0) for k in range(len(nodes)))
                eng_temp = sum(((node.a + (etime if k == j else 0)) / 1000 * node.pmax + (M_temp / 1000 - (node.a + (etime if k == j else 0)) / 1000) * node.pmin) for k, node in enumerate(nodes))
                nodes[j].engCons = eng_temp
                nodes[j].rcl = 1 if resp <= T[i].d else 0
                if nodes[j].rcl == 1:
                    rcl.append(nodes[j])
            if not rcl:
                index = random.randint(0, len(nodes)-1)
            else:
                rcl.sort(key=lambda n: n.engCons)
                x = max(int(0.4 * len(rcl)), 1)
                y = random.randint(0, x-1)
                index = rcl[y].id
            etime = T[i].s / nodes[index].c * 1000
            T[i].resp = nodes[index].a + etime + nodes[index].d + nodes[index].dis * (T[i].in_ + T[i].out) / nodes[index].b
            nodes[index].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / NRUN, sum_num_sat / NRUN, sum_pdst / NRUN, sum_eng / NRUN, sum_cost / NRUN, sum_pen / 1000 / NRUN]

# semi_greedy_multistart_old
def semi_greedy_multistart_old(ntask, nfog, ncloud):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    num_itr = 100
    for _ in range(NRUN):
        T_base, nodes_base = generate_for_old_models(ntask, nfog, ncloud)
        best_pdst = 0
        best_m = best_num_sat = best_eng = best_pen = best_cost = 0
        for itr in range(num_itr):
            T = [copy.deepcopy(t) for t in T_base]
            nodes = [copy.deepcopy(n) for n in nodes_base]
            for node in nodes:
                node.a = node.ms = node.eng = 0
            for t in T:
                t.resp = 0
            T = sorted(T, key=lambda t: t.d)
            for i in range(ntask):
                rcl = []
                for j in range(len(nodes)):
                    etime = T[i].s / nodes[j].c * 1000
                    resp = nodes[j].a + etime + nodes[j].d + nodes[j].dis * (T[i].in_ + T[i].out) / nodes[j].b
                    M_temp = max(node.a + (etime if k == j else 0) for k in range(len(nodes)))
                    eng_temp = sum(((node.a + (etime if k == j else 0)) / 1000 * node.pmax + (M_temp / 1000 - (node.a + (etime if k == j else 0)) / 1000) * node.pmin) for k, node in enumerate(nodes))
                    nodes[j].engCons = eng_temp
                    nodes[j].rcl = 1 if resp <= T[i].d else 0
                    if nodes[j].rcl == 1:
                        rcl.append(nodes[j])
                if not rcl:
                    index = random.randint(0, len(nodes)-1)
                else:
                    rcl.sort(key=lambda n: n.engCons)
                    x = max(int(0.4 * len(rcl)), 1)
                    y = random.randint(0, x-1)
                    index = rcl[y].id
                etime = T[i].s / nodes[index].c * 1000
                T[i].resp = nodes[index].a + etime + nodes[index].d + nodes[index].dis * (T[i].in_ + T[i].out) / nodes[index].b
                nodes[index].a += etime
            m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
            if pdst > best_pdst:
                best_pdst = pdst
                best_m = m
                best_num_sat = num_sat
                best_eng = eng
                best_pen = pen
                best_cost = cost
        sum_m += best_m
        sum_num_sat += best_num_sat
        sum_pdst += best_pdst
        sum_eng += best_eng
        sum_pen += best_pen
        sum_cost += best_cost
    return [sum_m / 1000 / NRUN, sum_num_sat / NRUN, sum_pdst / NRUN, sum_eng / NRUN, sum_cost / NRUN, sum_pen / 1000 / NRUN]

ntask = NUM_TASKS
nfog = NUM_FOG_NODES
ncloud = NUM_CLOUD_NODES
results = {}
results['FCFS'] = fcfs_old(ntask, nfog, ncloud)
results['EDF'] = edf_old(ntask, nfog, ncloud)
results['GfE'] = gfe_old(ntask, nfog, ncloud)
results['Detour'] = detour_old(ntask, nfog, ncloud)
results['PSG'] = semi_greedy_old(ntask, nfog, ncloud)
results['PSG-M'] = semi_greedy_multistart_old(ntask, nfog, ncloud)
sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0.0
for N_RUN in range(NRUN):
    tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix = generate_parameters()
    _, best_alloc = mofaro(tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix)
    est, lft, exec_times, complete_times = calculate_exec_times(best_alloc, tasks, nodes, G, bw_matrix, lat_matrix)
    m = max(complete_times) if complete_times else 0
    num_sat = sum(1 for ct, t in zip(complete_times, tasks) if ct <= t['d'])
    pdst = num_sat / len(tasks) if tasks else 0
    node_a = [0.0] * len(nodes)
    best_alloc_list=[]
    tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix = generate_parameters()
    best_objs, best_alloc = mofaro(tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix)
    best_alloc_list.append(best_alloc)
    for i in range(len(tasks)):
      j = np.argmax(best_alloc[i])
      node_a[j] += exec_times[i]
    eng_cons = sum((a / 1000 * nodes[jj]['pmax']) + ((m / 1000 - a / 1000) * nodes[jj]['pmin']) for jj, a in enumerate(node_a))
    total_penalty = sum(max(0, ct - t['d']) * t['p'] for ct, t in zip(complete_times, tasks))
    total_cost = sum(node_a[jj] / 1000 * nodes[jj]['pc'] for jj in range(len(nodes)))
    sum_m += m
    sum_num_sat += num_sat
    sum_pdst += pdst
    sum_eng += eng_cons
    sum_pen += total_penalty
    sum_cost += total_cost
    results['MOFARO'] = [sum_m / 1000 / NRUN, sum_num_sat / NRUN, sum_pdst / NRUN, sum_eng / NRUN, sum_cost / NRUN, sum_pen / 1000 / NRUN]
    print(f"\n************* Table of Results: NRUN= {N_RUN} for MOFARO and NRUN=10 for other Methods*************")
    print("| Method | Makespan (s) | numSat | PDST | engCons (J) | Cost | totPenalty (s) |")
    res = results['MOFARO']
    print(f"| MOFARO | {res[0]:.2f} | {res[1]:.0f} | {res[2]:.4f} | {res[3]:.0f} | {res[4]:.2f} | {res[5]:.2f} |")

results['MOFARO'] = [sum_m / 1000 / NRUN, sum_num_sat / NRUN, sum_pdst / NRUN, sum_eng / NRUN, sum_cost / NRUN, sum_pen / 1000 / NRUN]
print("\n************* Table of Results*************")
print("| Method | Makespan (s) | numSat | PDST | engCons (J) | Cost | totPenalty (s) |")
for model, res in results.items():
  print(f"| {model} | {res[0]:.2f} | {res[1]:.0f} | {res[2]:.4f} | {res[3]:.0f} | {res[4]:.2f} | {res[5]:.2f} |")

print("\n************* Results*************")
for model, res in results.items():
  print(model)
  print(f"Makespan (s): {res[0]:.2f}")
  print(f"numSat: {res[1]:.0f}")
  print(f"PDST: {res[2]:.4f}")
  print(f"engCons (J): {res[3]:.0f}")
  print(f"Cost: {res[4]:.2f}")
  print(f"totPenalty (s): {res[5]:.2f}")
  print("---------------------------")

# Draw charts
models = list(results.keys())
metrics = ['Makespan (s)', 'numSat', 'PDST', 'engCons (J)', 'Cost', 'totPenalty (s)']
for i, metric in enumerate(metrics):
  values = [res[i] for res in results.values()]
  plt.figure(figsize=(10, 5))
  plt.bar(models, values)
  plt.title(metric)
  plt.xlabel('Methods')
  plt.ylabel(metric)
  plt.xticks(rotation=45)
  plt.tight_layout()
  plt.show()

# Average objectives over runs
print("\nSample Allocation from last run:")
for i in range(NUM_TASKS):
    j = np.argmax(best_alloc_list[-1][i])
    print(f"Task {i} allocated to Node {nodes[j]['id']} ({nodes[j]['layer']})")
print("\nUpdated Node Resources (example for Node 0): cap", nodes[0]['available_c'], "mem", nodes[0]['available_m'])

import numpy as np
import networkx as nx
from scipy.stats import levy_stable
import random
import math
import copy
import time
import matplotlib.pyplot as plt
# Constants (optimized, edge only for MOFARO)
NUM_TASKS = 50
NUM_EDGE_NODES = 30  # For MOFARO
NUM_FOG_NODES = 5
NUM_CLOUD_NODES = 1
POP_SIZE = 5
MAX_IT = 5
LEVY_BETA = 1.5
P_LOSS_MIN = 0.05
P_LOSS_MAX = 0.1
BETA_ENERGY = 1e-12
P_TX = 0.1
C_DVFS = 1e-9
V_DVFS = 1.2
F_DVFS = 1.0
OVERHEAD_TIME = 0.1
OVERHEAD_COST = 0.01
W_QOS = 1.0
MARKOV_STATES = 2
TRANS_PROB = 0.8
ALPHA = 0.2
NRUN = 10
FTC_DELAY = random.randint(30, 300)
MAX_Q = 3
N_USERS = 100
FRACTAL_DIM_MIN = 1.5
FRACTAL_DIM_MAX = 2.0
# Generate parameters for MOFARO (with edge)
def generate_parameters():
    tasks = []
    for i in range(NUM_TASKS):
        s = random.randint(100, 500)
        dl = random.randint(100, 1000)
        task = {
            'id': i,
            's': s,
            'm': random.randint(50, 200),
            'd': dl,
            'p': random.uniform(0.01, 0.5),
            'q': random.uniform(1, MAX_Q),
            'in_': random.randint(100, 1000),
            'out': random.randint(1, 100),
            'resp': 0
        }
        tasks.append(task)
    nodes = []
    for j in range(NUM_EDGE_NODES + NUM_FOG_NODES + NUM_CLOUD_NODES):
        layer = 'edge' if j < NUM_EDGE_NODES else 'fog' if j < NUM_EDGE_NODES + NUM_FOG_NODES else 'cloud'
        c = random.randint(1000, 5000)
        m = random.randint(512, 4096)
        b = 500
        dis = random.randint(1, 3)
        pc = random.uniform(0.05, 0.2)
        d = dis * random.randint(1, 2)
        pmax = random.randint(50, 100)
        pmin = random.uniform(0.5, 0.6) * pmax
        node = {
            'id': j,
            'layer': layer,
            'c': c,
            'm': m,
            'b': b,
            'pc': pc,
            'd': d,
            'dis': dis,
            'pmax': pmax,
            'pmin': pmin,
            'a': 0,
            'ms': 0,
            'eng': 0,
            'rcl': 0,
            'available_c': c,
            'available_m': m,
            'engCons': 0,
            'resp': 0
        }
        nodes.append(node)
    bw_matrix = np.random.uniform(100, 500, (len(nodes), len(nodes)))
    lat_matrix = np.random.uniform(10, 100, (len(nodes), len(nodes)))
    np.fill_diagonal(bw_matrix, np.inf)
    np.fill_diagonal(lat_matrix, 0)
    dist_matrix = np.random.uniform(10, 100, (len(nodes), len(nodes)))
    np.fill_diagonal(dist_matrix, 0)
    G = nx.DiGraph()
    for i in range(NUM_TASKS):
        G.add_node(i)
    if NUM_TASKS > 1:
        G.add_edge(0, 1, weight=random.uniform(1, 10))
    transition_matrix = np.full((MARKOV_STATES, MARKOV_STATES), (1 - TRANS_PROB) / (MARKOV_STATES - 1))
    np.fill_diagonal(transition_matrix, TRANS_PROB)
    return tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix
# t_proc
def t_proc(task_s, node_c):
    base = (task_s / node_c) * 1000
    return base + OVERHEAD_TIME * base
# t_comm
def t_comm(task_d, j, k, bw_matrix, lat_matrix, nodes):
    if j == k:
        return 0
    p_loss = random.uniform(P_LOSS_MIN, P_LOSS_MAX)
    bw = bw_matrix[j, k]
    if bw == 0:
        bw = 1e-6
    delay = lat_matrix[j, k]
    if nodes[j]['layer'] == 'fog' and nodes[k]['layer'] == 'cloud':
        delay += FTC_DELAY
    return (task_d / bw) / (1 - p_loss) + delay
# calculate_exec_times
def calculate_exec_times(alloc, tasks, nodes, G, bw_matrix, lat_matrix):
    num_tasks = len(tasks)
    num_nodes = len(nodes)
    est = [0.0] * num_tasks
    exec_times = [0.0] * num_tasks
    complete_times = [0.0] * num_tasks
    node_finish = [0.0] * num_nodes
    topo_order = list(nx.topological_sort(G))
    for i in topo_order:
        j = np.argmax(alloc[i])
        ready_time = 0.0
        for pred in G.predecessors(i):
            k = np.argmax(alloc[pred])
            comm_sum = sum(t_comm(tasks[pred]['out'], k, j, bw_matrix, lat_matrix, nodes) for _ in range(N_USERS))
            avg_comm = comm_sum / N_USERS if k != j else 0.0
            ready_time = max(ready_time, complete_times[pred] + avg_comm)
        est[i] = max(ready_time, node_finish[j])
        exec_times[i] = t_proc(tasks[i]['s'], nodes[j]['c'])
        complete_times[i] = est[i] + exec_times[i]
        node_finish[j] = complete_times[i]
    lft = [math.inf] * num_tasks
    rev_topo = list(reversed(topo_order))
    for i in rev_topo:
        min_succ = tasks[i]['d']
        for succ in G.successors(i):
            k = np.argmax(alloc[succ])
            j = np.argmax(alloc[i])
            comm_sum = sum(t_comm(tasks[i]['out'], j, k, bw_matrix, lat_matrix, nodes) for _ in range(N_USERS))
            avg_comm = comm_sum / N_USERS if j != k else 0.0
            min_succ = min(min_succ, lft[succ] - exec_times[succ] - avg_comm)
        lft[i] = min_succ
    return est, lft, exec_times, complete_times
# mob_constraint
def mob_constraint(task, transition_matrix):
    pred_mob = 0
    for u in range(N_USERS):
        path_prob = 1.0
        current_state = random.randint(0, MARKOV_STATES-1)
        for _ in range(3):
            next_state = np.argmax(transition_matrix[current_state])
            path_prob *= transition_matrix[current_state, next_state]
            current_state = next_state
        cost_mob = random.uniform(0, 1)
        pred_mob += path_prob * cost_mob
    return pred_mob / N_USERS
# check_constraints
def check_constraints(alloc, tasks, nodes, G, complete_times):
    penalty = 0
    num_nodes = len(nodes)
    node_load_m = [0] * num_nodes
    for i in range(len(tasks)):
        j = np.argmax(alloc[i])
        node_load_m[j] += tasks[i]['m']
        if node_load_m[j] > nodes[j]['m']:
            penalty += 1000 * (node_load_m[j] - nodes[j]['m'])
        over = complete_times[i] - tasks[i]['d']
        if over > 0:
            penalty += 1500 * over * tasks[i]['p']
            if nodes[j]['layer'] == 'cloud':
                penalty += 500 * over
    return penalty
# z1
def z1(complete_times):
    return max(complete_times) if complete_times else 0
# z2
def z2(alloc, tasks, nodes, exec_times, dist_matrix, G, complete_times):
    energy = 0
    M = z1(complete_times)
    for i in range(len(tasks)):
        j = np.argmax(alloc[i])
        p_j = C_DVFS * V_DVFS**2 * F_DVFS
        e_proc = p_j * exec_times[i]
        energy += e_proc
        e_comm_sum = 0
        for succ in G.successors(i):
            k = np.argmax(alloc[succ])
            d_bits = tasks[i]['out'] * 8 * 1024
            for u in range(N_USERS):
                e_comm_sum += (BETA_ENERGY * d_bits + P_TX) * dist_matrix[j][k]
        energy += e_comm_sum / N_USERS
    node_a = [0.0] * len(nodes)
    for i in range(len(tasks)):
        j = np.argmax(alloc[i])
        node_a[j] += exec_times[i]
    for jj in range(len(nodes)):
        a = node_a[jj]
        energy += (a / 1000) * nodes[jj]['pmax'] + ((M / 1000) - (a / 1000)) * nodes[jj]['pmin']
    energy += 0.1 * energy
    return energy
# z3
def z3(alloc, tasks, nodes, exec_times):
    cost = 0
    for i in range(len(tasks)):
        j = np.argmax(alloc[i])
        c_proc = nodes[j]['pc'] * exec_times[i]
        c_net = 0.01 * tasks[i]['d']
        c_store = 0.001 * tasks[i]['d'] * exec_times[i]
        cost += c_proc + c_net + c_store + OVERHEAD_COST * c_proc
    return cost
# z4
def z4(tasks, complete_times):
    qos = 0
    max_q = MAX_Q
    for i in range(len(tasks)):
        qos += (1 - tasks[i]['q'] / max_q) * complete_times[i]
        qos += max(0, complete_times[i] - tasks[i]['d']) * W_QOS
    return qos
# evaluate_objs
def evaluate_objs(alloc, tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix, exec_times, complete_times):
    penalty = check_constraints(alloc, tasks, nodes, G, complete_times)
    z1_val = z1(complete_times) + penalty
    z2_val = z2(alloc, tasks, nodes, exec_times, dist_matrix, G, complete_times) + penalty
    z3_val = z3(alloc, tasks, nodes, exec_times) + penalty
    z4_val = z4(tasks, complete_times) + penalty
    return [z1_val, z2_val, z3_val, z4_val]
# dominates
def dominates(a, b):
    return all(a[k] <= b[k] for k in range(4)) and any(a[k] < b[k] for k in range(4))
# non_dominated_sort
def non_dominated_sort(objectives):
    front = []
    for i in range(len(objectives)):
        dominated = False
        for j in range(len(objectives)):
            if i != j and dominates(objectives[j], objectives[i]):
                dominated = True
                break
        if not dominated:
            front.append(i)
    return front
# crowding_distance
def crowding_distance(objectives):
    if len(objectives) < 2:
        return [math.inf] * len(objectives)
    dist = [0] * len(objectives)
    for m in range(4):
        sorted_idx = np.argsort([obj[m] for obj in objectives])
        min_val = objectives[sorted_idx[0]][m]
        max_val = objectives[sorted_idx[-1]][m]
        dist[sorted_idx[0]] = math.inf
        dist[sorted_idx[-1]] = math.inf
        for i in range(1, len(objectives)-1):
            dist[sorted_idx[i]] += (objectives[sorted_idx[i+1]][m] - objectives[sorted_idx[i-1]][m]) / (max_val - min_val + 1e-10)
    return dist
# topsis
def topsis(front_objs):
    m = len(front_objs)
    if m == 0:
        return [math.inf] * 4
    n = 4
    decision_matrix = np.array(front_objs)
    normalized = decision_matrix / np.sqrt(np.sum(decision_matrix**2, axis=0))
    weights = np.ones(n) / n
    weighted = normalized * weights
    ideal = np.min(weighted, axis=0)
    anti_ideal = np.max(weighted, axis=0)
    sep_pos = np.sqrt(np.sum((weighted - ideal)**2, axis=1))
    sep_neg = np.sqrt(np.sum((weighted - anti_ideal)**2, axis=1))
    closeness = sep_neg / (sep_pos + sep_neg + 1e-10)
    best_idx = np.argmax(closeness)
    return front_objs[best_idx]
# update_resources
def update_resources(best_alloc, tasks, nodes, exec_times):
    for i in range(len(tasks)):
        j = np.argmax(best_alloc[i])
        nodes[j]['available_m'] -= tasks[i]['m']
        nodes[j]['available_m'] = max(0, nodes[j]['available_m'])
        nodes[j]['a'] += exec_times[i]
# handle_failure
def handle_failure(task, nodes):
    available = [n for n in nodes if n['available_m'] > task['m']]
    if available:
        return random.choice(available)['id']
    return random.randint(0, len(nodes)-1)
# sort_tasks
def sort_tasks(tasks):
    return sorted(tasks, key=lambda t: t['d'])
# sort_nodes_eng
def sort_nodes_eng(nodes, rcl_only=False):
    filtered = [n for n in nodes if not rcl_only or n['rcl'] == 1]
    sorted_nodes = sorted(filtered, key=lambda n: n['engCons'] + (0 if n['layer'] in ['edge', 'fog'] else 10))
    return sorted_nodes
# mofaro
def mofaro(tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix):
    num_tasks = len(tasks)
    num_nodes = len(nodes)
    tasks = sort_tasks(tasks)
    population = []
    for _ in range(POP_SIZE):
        alloc = np.zeros((num_tasks, num_nodes))
        for i in range(num_tasks):
            j = random.randint(0, num_nodes-1)
            alloc[i, j] = 1
        population.append(alloc)
    for it in range(MAX_IT):
        objectives = []
        for p_idx, alloc in enumerate(population):
            est, lft, exec_times, complete_times = calculate_exec_times(alloc, tasks, nodes, G, bw_matrix, lat_matrix)
            objs = evaluate_objs(alloc, tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix, exec_times, complete_times)
            objectives.append(objs)
        front_indices = non_dominated_sort(objectives)
        front_objs = [objectives[idx] for idx in front_indices if idx < len(objectives)]
        if len(front_objs) == 0:
            front_objs = objectives
            front_indices = list(range(len(objectives)))
        crowd_dist = crowding_distance(front_objs)
        sorted_front = sorted(range(len(front_objs)), key=lambda k: crowd_dist[k], reverse=True)
        selected = min(POP_SIZE, len(front_indices))
        selected_indices = [front_indices[s] for s in sorted_front[:selected]]
        new_pop = []
        e = 2 * (1 - it / MAX_IT)
        for p_idx in range(POP_SIZE):
            alloc = population[p_idx]
            for i in range(num_tasks):
                nodes_copy = copy.deepcopy(nodes)
                for j in range(num_nodes):
                    temp_alloc = alloc.copy()
                    temp_alloc[i] = np.zeros(num_nodes)
                    temp_alloc[i, j] = 1
                    _, _, _, complete_temp = calculate_exec_times(temp_alloc, tasks, nodes_copy, G, bw_matrix, lat_matrix)
                    nodes_copy[j]['resp'] = complete_temp[i]
                    if complete_temp[i] <= tasks[i]['d']:
                        nodes_copy[j]['rcl'] = 1
                    else:
                        nodes_copy[j]['rcl'] = 0
                    M = max(complete_temp)
                    engCons = 0
                    for k in range(num_nodes):
                        node_ms = random.uniform(0, M)
                        node_eng = (node_ms / 1000) * nodes_copy[k]['pmax'] + ((M / 1000) - (node_ms / 1000)) * nodes_copy[k]['pmin']
                        engCons += node_eng
                    nodes_copy[j]['engCons'] = engCons
                rcl_nodes = [n for n in nodes_copy if n['rcl'] == 1]
                rcl_size = len(rcl_nodes)
                if rcl_size == 0:
                    j = handle_failure(tasks[i], nodes)
                else:
                    x = int(ALPHA * rcl_size)
                    if x < 1:
                        x = 1
                    sorted_rcl = sort_nodes_eng(rcl_nodes)
                    y = random.randint(0, x-1)
                    j = sorted_rcl[y]['id']
                alloc[i] = np.zeros(num_nodes)
                alloc[i, j] = 1
            if random.random() < e:
                # Fractal-based exploration
                fractal_dim = random.uniform(FRACTAL_DIM_MIN, FRACTAL_DIM_MAX)
                theta = random.uniform(0, 2 * math.pi)
                r = random.random()
                fractal_step = math.sin(theta * fractal_dim) / (r ** LEVY_BETA)
                levy_s = levy_stable.rvs(alpha=LEVY_BETA, beta=1.0, size=num_tasks * num_nodes).reshape((num_tasks, num_nodes))
                levy_s = np.clip(levy_s, -1, 1)
                fractal_s = fractal_step * levy_s
                best_alloc = population[random.choice(selected_indices)]
                new_alloc = alloc + fractal_s * (best_alloc - alloc)
            else:
                global_best = population[selected_indices[0]]
                new_alloc = alloc + random.random() * (global_best - alloc)
            for i in range(num_tasks):
                row = new_alloc[i]
                j = np.argmax(row)
                new_alloc[i] = np.zeros(num_nodes)
                new_alloc[i, j] = 1
            for i in range(num_tasks):
                mob = random.uniform(0, 1)
                pred_mob = mob_constraint(tasks[i], transition_matrix)
                if mob > pred_mob:
                    new_alloc[i] = np.zeros(num_nodes)
                    new_alloc[i, handle_failure(tasks[i], nodes)] = 1
            new_pop.append(new_alloc)
        population = new_pop
    objectives = []
    for alloc in population:
        est, lft, exec_times, complete_times = calculate_exec_times(alloc, tasks, nodes, G, bw_matrix, lat_matrix)
        objs = evaluate_objs(alloc, tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix, exec_times, complete_times)
        objectives.append(objs)
    front_indices = non_dominated_sort(objectives)
    front_objs = [objectives[idx] for idx in front_indices if idx < len(objectives)]
    if len(front_objs) == 0:
        front_objs = objectives
    best_objs = topsis(front_objs)
    best_front_idx = np.argmin([sum(o) for o in front_objs])
    best_alloc = population[front_indices[best_front_idx] if len(front_indices) > 0 else 0]
    _, _, exec_times, _ = calculate_exec_times(best_alloc, tasks, nodes, G, bw_matrix, lat_matrix)
    update_resources(best_alloc, tasks, nodes, exec_times)
    return best_objs, best_alloc
# Old models classes
class TaskOld:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)
class NodeOld:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)
# generate_for_old_models (no edge for old models)
def generate_for_old_models(ntask, nfog, ncloud):
    tasks = []
    for i in range(ntask):
        x = random.randint(0, 1)
        if x == 0:
            s = random.randint(100, 372)
            d = random.randint(500, 2500)
        else:
            s = random.randint(1028, 4280)
            d = random.randint(500, 2500)
        m = random.randint(50, 200)
        p = random.uniform(0.01, 0.5)
        q = random.uniform(90, 99.99)
        in_ = random.randint(100, 10000)
        out = random.randint(1, 991)
        tasks.append(TaskOld(id=i, s=s, m=m, d=d, p=p, q=q, in_=in_, out=out, resp=0))
    nodes = []
    for j in range(nfog):
        c = random.randint(2000, 6000)
        m = random.randint(150, 250)
        b = 1000
        dis = random.randint(1, 5)
        pc = random.uniform(0.1, 0.4)
        d = dis * random.randint(1, 3)
        pmax = random.randint(80, 200)
        pmin = random.uniform(0.6, 0.7) * pmax
        nodes.append(NodeOld(id=j, layer='fog', c=c, m=m, b=b, dis=dis, pc=pc, d=d, pmax=pmax, pmin=pmin, a=0, ms=0, eng=0, rcl=0, engCons=0, resp=0))
    x = random.randint(200, 500)
    for j in range(nfog, nfog + ncloud):
        c = random.randint(3000, 5000)
        m = random.randint(8192, 65536)
        b = random.randint(100, 10000)
        pc = random.uniform(0.7, 1.0)
        d = x
        pmax = random.randint(200, 400)
        pmin = 0.6 * pmax
        nodes.append(NodeOld(id=j, layer='cloud', c=c, m=m, b=b, dis=dis, pc=pc, d=d, pmax=pmax, pmin=pmin, a=0, ms=0, eng=0, rcl=0, engCons=0, resp=0))
    return tasks, nodes
# compute_metrics_old
def compute_metrics_old(T, nodes, ntask):
    all_nodes = nodes
    total_penalty = sum(max(0, T[i].resp - T[i].d) for i in range(ntask))
    num_sat = sum(1 for i in range(ntask) if T[i].resp <= T[i].d)
    M = max(node.a for node in all_nodes) if all_nodes else 0
    eng_cons = sum((node.a / 1000 * node.pmax) + ((M / 1000 - node.a / 1000) * node.pmin) for node in all_nodes)
    total_cost = sum(node.a / 1000 * node.pc for node in all_nodes)
    pdst = num_sat / ntask if ntask > 0 else 0
    return M, num_sat, pdst, eng_cons, total_penalty, total_cost
# fcfs_old
def fcfs_old(ntask, nfog, ncloud, nrun):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(nrun):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        for i in range(ntask):
            x = random.randint(0, len(nodes)-1)
            etime = T[i].s / nodes[x].c * 1000
            T[i].resp = nodes[x].a + etime + nodes[x].d + nodes[x].dis * (T[i].in_ + T[i].out) / nodes[x].b
            nodes[x].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / nrun, sum_num_sat / nrun, sum_pdst / nrun, sum_eng / nrun, sum_cost / nrun, sum_pen / 1000 / nrun]
# edf_old
def edf_old(ntask, nfog, ncloud, nrun):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(nrun):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        T = sorted(T, key=lambda t: t.d)
        for i in range(ntask):
            x = random.randint(0, len(nodes)-1)
            etime = T[i].s / nodes[x].c * 1000
            T[i].resp = nodes[x].a + etime + nodes[x].d + nodes[x].dis * (T[i].in_ + T[i].out) / nodes[x].b
            nodes[x].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / nrun, sum_num_sat / nrun, sum_pdst / nrun, sum_eng / nrun, sum_cost / nrun, sum_pen / 1000 / nrun]
# gfe_old
def gfe_old(ntask, nfog, ncloud, nrun):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(nrun):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        for i in range(ntask):
            min_eng = float('inf')
            index = 0
            for j in range(len(nodes)):
                etime = T[i].s / nodes[j].c * 1000
                resp = nodes[j].a + etime + nodes[j].d + nodes[j].dis * (T[i].in_ + T[i].out) / nodes[j].b
                M_temp = max(node.a + (etime if k == j else 0) for k in range(len(nodes)))
                eng_temp = sum(((node.a + (etime if k == j else 0)) / 1000 * node.pmax + (M_temp / 1000 - (node.a + (etime if k == j else 0)) / 1000) * node.pmin) for k, node in enumerate(nodes))
                if eng_temp < min_eng:
                    min_eng = eng_temp
                    index = j
            etime = T[i].s / nodes[index].c * 1000
            T[i].resp = nodes[index].a + etime + nodes[index].d + nodes[index].dis * (T[i].in_ + T[i].out) / nodes[index].b
            nodes[index].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / nrun, sum_num_sat / nrun, sum_pdst / nrun, sum_eng / nrun, sum_cost / nrun, sum_pen / 1000 / nrun]
# detour_old
def detour_old(ntask, nfog, ncloud, nrun):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(nrun):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        for i in range(ntask):
            min_resp = float('inf')
            index = 0
            for j in range(len(nodes)):
                etime = T[i].s / nodes[j].c * 1000
                resp = nodes[j].a + etime + nodes[j].d + nodes[j].dis * (T[i].in_ + T[i].out) / nodes[j].b
                if resp < min_resp:
                    min_resp = resp
                    index = j
            etime = T[i].s / nodes[index].c * 1000
            T[i].resp = min_resp
            nodes[index].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / nrun, sum_num_sat / nrun, sum_pdst / nrun, sum_eng / nrun, sum_cost / nrun, sum_pen / 1000 / nrun]
# semi_greedy_old
def semi_greedy_old(ntask, nfog, ncloud, nrun):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    for _ in range(nrun):
        T, nodes = generate_for_old_models(ntask, nfog, ncloud)
        for node in nodes:
            node.a = node.ms = node.eng = 0
        for t in T:
            t.resp = 0
        T = sorted(T, key=lambda t: t.d)
        for i in range(ntask):
            rcl = []
            for j in range(len(nodes)):
                etime = T[i].s / nodes[j].c * 1000
                resp = nodes[j].a + etime + nodes[j].d + nodes[j].dis * (T[i].in_ + T[i].out) / nodes[j].b
                M_temp = max(node.a + (etime if k == j else 0) for k in range(len(nodes)))
                eng_temp = sum(((node.a + (etime if k == j else 0)) / 1000 * node.pmax + (M_temp / 1000 - (node.a + (etime if k == j else 0)) / 1000) * node.pmin) for k, node in enumerate(nodes))
                nodes[j].engCons = eng_temp
                nodes[j].rcl = 1 if resp <= T[i].d else 0
                if nodes[j].rcl == 1:
                    rcl.append(nodes[j])
            if not rcl:
                index = random.randint(0, len(nodes)-1)
            else:
                rcl.sort(key=lambda n: n.engCons)
                x = max(int(0.4 * len(rcl)), 1)
                y = random.randint(0, x-1)
                index = rcl[y].id
            etime = T[i].s / nodes[index].c * 1000
            T[i].resp = nodes[index].a + etime + nodes[index].d + nodes[index].dis * (T[i].in_ + T[i].out) / nodes[index].b
            nodes[index].a += etime
        m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng
        sum_pen += pen
        sum_cost += cost
    return [sum_m / 1000 / nrun, sum_num_sat / nrun, sum_pdst / nrun, sum_eng / nrun, sum_cost / nrun, sum_pen / 1000 / nrun]
# semi_greedy_multistart_old
def semi_greedy_multistart_old(ntask, nfog, ncloud, nrun):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0
    num_itr = 100
    for _ in range(nrun):
        T_base, nodes_base = generate_for_old_models(ntask, nfog, ncloud)
        best_pdst = 0
        best_m = best_num_sat = best_eng = best_pen = best_cost = 0
        for itr in range(num_itr):
            T = [copy.deepcopy(t) for t in T_base]
            nodes = [copy.deepcopy(n) for n in nodes_base]
            for node in nodes:
                node.a = node.ms = node.eng = 0
            for t in T:
                t.resp = 0
            T = sorted(T, key=lambda t: t.d)
            for i in range(ntask):
                rcl = []
                for j in range(len(nodes)):
                    etime = T[i].s / nodes[j].c * 1000
                    resp = nodes[j].a + etime + nodes[j].d + nodes[j].dis * (T[i].in_ + T[i].out) / nodes[j].b
                    M_temp = max(node.a + (etime if k == j else 0) for k in range(len(nodes)))
                    eng_temp = sum(((node.a + (etime if k == j else 0)) / 1000 * node.pmax + (M_temp / 1000 - (node.a + (etime if k == j else 0)) / 1000) * node.pmin) for k, node in enumerate(nodes))
                    nodes[j].engCons = eng_temp
                    nodes[j].rcl = 1 if resp <= T[i].d else 0
                    if nodes[j].rcl == 1:
                        rcl.append(nodes[j])
                if not rcl:
                    index = random.randint(0, len(nodes)-1)
                else:
                    rcl.sort(key=lambda n: n.engCons)
                    x = max(int(0.4 * len(rcl)), 1)
                    y = random.randint(0, x-1)
                    index = rcl[y].id
                etime = T[i].s / nodes[index].c * 1000
                T[i].resp = nodes[index].a + etime + nodes[index].d + nodes[index].dis * (T[i].in_ + T[i].out) / nodes[index].b
                nodes[index].a += etime
            m, num_sat, pdst, eng, pen, cost = compute_metrics_old(T, nodes, ntask)
            if pdst > best_pdst:
                best_pdst = pdst
                best_m = m
                best_num_sat = num_sat
                best_eng = eng
                best_pen = pen
                best_cost = cost
        sum_m += best_m
        sum_num_sat += best_num_sat
        sum_pdst += best_pdst
        sum_eng += best_eng
        sum_pen += best_pen
        sum_cost += best_cost
    return [sum_m / 1000 / nrun, sum_num_sat / nrun, sum_pdst / nrun, sum_eng / nrun, sum_cost / nrun, sum_pen / 1000 / nrun]
def mofaro_avg(ntask, nfog, ncloud, nrun):
    sum_m = sum_num_sat = sum_pdst = sum_eng = sum_pen = sum_cost = 0.0
    for _ in range(nrun):
        tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix = generate_parameters()
        best_objs, best_alloc = mofaro(tasks, nodes, G, bw_matrix, lat_matrix, dist_matrix, transition_matrix)
        est, lft, exec_times, complete_times = calculate_exec_times(best_alloc, tasks, nodes, G, bw_matrix, lat_matrix)
        m = max(complete_times) if complete_times else 0
        num_sat = sum(1 for ct, t in zip(complete_times, tasks) if ct <= t['d'])
        pdst = num_sat / len(tasks) if tasks else 0
        node_a = [0.0] * len(nodes)
        for i in range(len(tasks)):
            j = np.argmax(best_alloc[i])
            node_a[j] += exec_times[i]
        eng_cons = sum((a / 1000 * nodes[jj]['pmax']) + ((m / 1000 - a / 1000) * nodes[jj]['pmin']) for jj, a in enumerate(node_a))
        total_penalty = sum(max(0, ct - t['d']) * t['p'] for ct, t in zip(complete_times, tasks))
        total_cost = sum(node_a[jj] / 1000 * nodes[jj]['pc'] for jj in range(len(nodes)))
        sum_m += m
        sum_num_sat += num_sat
        sum_pdst += pdst
        sum_eng += eng_cons
        sum_pen += total_penalty
        sum_cost += total_cost
    return [sum_m / 1000 / nrun, sum_num_sat / nrun, sum_pdst / nrun, sum_eng / nrun, sum_cost / nrun, sum_pen / 1000 / nrun]
# Main
ntask = NUM_TASKS
nfog = NUM_FOG_NODES
ncloud = NUM_CLOUD_NODES
nrun_values = [1, 5, 10]
algorithms = ['FCFS', 'EDF', 'GfE', 'Detour', 'PSG', 'PSG-M', 'MOFARO']
metrics = ['Makespan (s)', 'numSat', 'PDST', 'engCons (J)', 'Cost', 'totPenalty (s)']
data = {metric: {alg: [] for alg in algorithms} for metric in metrics}
for n in nrun_values:
    results = {}
    results['FCFS'] = fcfs_old(ntask, nfog, ncloud, n)
    results['EDF'] = edf_old(ntask, nfog, ncloud, n)
    results['GfE'] = gfe_old(ntask, nfog, ncloud, n)
    results['Detour'] = detour_old(ntask, nfog, ncloud, n)
    results['PSG'] = semi_greedy_old(ntask, nfog, ncloud, n)
    results['PSG-M'] = semi_greedy_multistart_old(ntask, nfog, ncloud, n)
    results['MOFARO'] = mofaro_avg(ntask, nfog, ncloud, n)
    for i, metric in enumerate(metrics):
        for alg in algorithms:
            data[metric][alg].append(results[alg][i])
import io
import base64
for metric in metrics:
    fig, ax = plt.subplots(figsize=(10, 6))
    for alg in algorithms:
        ax.plot(nrun_values, data[metric][alg], label=alg)
    ax.set_xlabel('NRUN')
    ax.set_ylabel(metric)
    ax.set_title(f'{metric} vs NRUN')
    ax.legend()
    buf = io.BytesIO()
    fig.savefig(buf, format='png')
    buf.seek(0)
    img_str = base64.b64encode(buf.read()).decode('utf-8')
    print(f'{metric} plot:')
    print('data:image/png;base64,' + img_str)
    plt.show()
    plt.close(fig)