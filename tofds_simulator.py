# -*- coding: utf-8 -*-
"""TOFDS-Simulator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AUQ0j6pOXO311f4p0hFxon-TsNdh3_r0
"""

import numpy as np
import random
import math
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque, namedtuple

# Parameters from the simulation
s = 3  # number of tasks
num_clusters = 2  # edge clusters
num_actions = num_clusters + 1  # +1 for cloud

nu_s = np.array([1e9, 1.1e9, 0.9e9])  # CPU cycles
kappa_s = np.array([1e6, 1.2e6, 0.8e6])  # input size bits
O_s = np.array([0.1e6, 0.15e6, 0.12e6])  # pre-trans data
r_s = np.array([[0,1,0], [1,0,1], [0,1,0]])  # relationship matrix
Y_s = np.array([[12, 13, 11], [14, 10, 15]])  # resources per cluster (extended to s=3)
B_q = 37.5e6  # bandwidth Hz
N0 = 1e-20  # noise W
tr = 0.1  # transmit power W
g_q = 1e-4  # channel gain
phi_LAN = 100e6  # bits/s
phi_WAN = 50e6  # bits/s
P_crs = 5  # power W
L_max = 10  # s
E_max = 50  # J
delta_Es = 0.05
delta_C = 0.05
delta_f = 0.05
f_hat = np.array([1.5e9, 1.8e9, 2.2e9])  # freq for clusters + cloud

# Compute xi_q using Shannon (Eq 7), same for all
xi_q = B_q * math.log2(1 + (tr * g_q) / N0)

# Weights
h_r = 0.5
h_b = 0.5
h_t = 0.5
h_e = 0.5

# AGE-MOEA params
POP_SIZE = 10
GENS = 5
CROSSOVER_PROB = 0.9
MUTATION_PROB = 1.0 / s

# DRL params
EPISODES = 50
BATCH_SIZE = 8
GAMMA = 0.99
EPS_START = 1.0
EPS_END = 0.01
EPS_DECAY = 0.995
TARGET_UPDATE = 10
MEMORY_SIZE = 10000
LR = 0.001
PENALTY = 10  # varrho

# Stage 1: AGE-MOEA for cluster selection

def evaluate_individual(v_s, num_clusters):
    Y_eq = np.zeros(num_clusters)
    for q in range(num_clusters):
        Lambda = np.where(v_s == q)[0]
        card_Lambda = len(Lambda)
        sum_y = np.sum(Y_s[q]) * (1 + delta_Es)
        Y_eq[q] = card_Lambda / sum_y if sum_y > 0 else 0
    y_ave = np.mean(Y_eq)
    b_total = np.sqrt(np.mean((Y_eq - y_ave)**2))
    return y_ave, b_total

def fast_non_dominated_sort(fitness):
    # Simplified: return indices of front1 (best)
    fronts = []
    for i in range(len(fitness)):
        dominated = False
        for j in range(len(fitness)):
            if all(fitness[j] <= fitness[i]) and any(fitness[j] < fitness[i]):
                dominated = True
                break
        if not dominated:
            fronts.append(i)
    return fronts[:POP_SIZE // 2]  # approximate front1

def normalize(front_fitness):
    min_vals = np.min(front_fitness, axis=0)
    max_vals = np.max(front_fitness, axis=0)
    return (front_fitness - min_vals) / (max_vals - min_vals + 1e-8)

def compute_lp(front_norm):
    theta = 2  # objectives
    mid = np.mean(front_norm, axis=0)
    sum_mid = np.sum(mid)
    if sum_mid == 0:
        return 2.0
    p = math.log(theta) / (math.log(theta) - math.log(sum_mid / theta))
    return max(1.0, min(p, 100))  # clamp

def survival_score(front_norm, p):
    scores = np.zeros(len(front_norm))
    for i in range(len(front_norm)):
        dists = np.inf
        for j in range(len(front_norm)):
            if i != j:
                dist = np.linalg.norm(front_norm[i] - front_norm[j], ord=p)
                dists = min(dists, dist)
        prx = np.linalg.norm(front_norm[i], ord=p)
        scores[i] = dists / prx if prx > 0 else 0
    return scores

def sbx_crossover(p1, p2):
    child = np.zeros_like(p1)
    for i in range(len(p1)):
        if random.random() < 0.5:
            child[i] = p1[i]
        else:
            child[i] = p2[i]
    return child

def polynomial_mutation(ind):
    for i in range(len(ind)):
        if random.random() < MUTATION_PROB:
            ind[i] = random.randint(0, num_clusters - 1)
    return ind

def age_moea():
    # Init population
    population = [np.random.randint(0, num_clusters, s) for _ in range(POP_SIZE)]
    for gen in range(GENS):
        fitness = []
        for ind in population:
            y_ave, b_total = evaluate_individual(ind, num_clusters)
            fitness.append(np.array([-y_ave, b_total]))  # max y_ave -> min -y_ave
        fitness = np.array(fitness)

        front1_idx = fast_non_dominated_sort(fitness)
        front_fitness = fitness[front1_idx]
        front_norm = normalize(front_fitness)

        p = compute_lp(front_norm)

        scores = survival_score(front_norm, p)

        # Select top
        selected_idx = np.argsort(scores)[-POP_SIZE // 2:]
        selected = [population[front1_idx[i]] for i in selected_idx]

        # Offspring
        offspring = []
        for _ in range(POP_SIZE // 2):
            p1, p2 = random.choices(selected, k=2)
            child = sbx_crossover(p1, p2)
            child = polynomial_mutation(child)
            offspring.append(child)

        population = selected + offspring

    # Best
    best_ind = population[0]
    best_y_ave, best_b_total = evaluate_individual(best_ind, num_clusters)
    print(f"Stage 1 Best V_s: {best_ind}, y_ave: {best_y_ave}, b_total: {best_b_total}")
    return best_ind

# Stage 2: Simplified Rainbow DRL (using Dueling DQN as base, with noise and prioritized replay approx)

Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class ReplayMemory:
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)

    def push(self, *args):
        self.memory.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory)

class DuelingDQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DuelingDQN, self).__init__()
        self.fc1 = nn.Linear(state_size, 128)
        self.value = nn.Linear(128, 1)
        self.advantage = nn.Linear(128, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        val = self.value(x)
        adv = self.advantage(x)
        return val + adv - adv.mean()

def get_state(v_s):
    flat_i = np.concatenate([nu_s, kappa_s, O_s])
    flat_r = r_s.flatten()
    flat_xi = np.full(s, xi_q) # corrected to use 's' instead of 'num_actions'
    state = np.concatenate([flat_i, flat_r, flat_xi, [delta_f, L_max, E_max]])
    return state

def compute_latency_energy(v_s):
    l_cmu = np.sum(kappa_s / xi_q)

    l_cpu = np.sum(nu_s / (f_hat[v_s] * (1 + delta_f)))
    for a in range(s):
        for b in range(s):
            if r_s[a, b] == 1 and a != b:
                if v_s[a] == v_s[b] and v_s[a] < num_clusters:  # same edge
                    m = np.inf
                elif v_s[a] < num_clusters and v_s[b] < num_clusters:  # diff edge
                    m = phi_LAN * (1 + delta_Es)
                else:  # cloud involved
                    m = phi_WAN * (1 + delta_C)
                l_cpu += O_s[a] / m if m != np.inf else 0

    l_total = l_cmu + l_cpu
    e_total = l_total * P_crs
    return l_total, e_total

def get_reward(l_total, e_total):
    norm_l = l_total / L_max
    norm_e = e_total / E_max
    r = - (h_t * norm_l + h_e * norm_e)
    if l_total > L_max or e_total > E_max:
        r -= PENALTY
    return r

def rainbow_drl(best_v_s):
    state_size = get_state(best_v_s).shape[0]
    policy_net = DuelingDQN(state_size, num_actions)
    target_net = DuelingDQN(state_size, num_actions)
    target_net.load_state_dict(policy_net.state_dict())

    optimizer = optim.Adam(policy_net.parameters(), lr=LR)
    memory = ReplayMemory(MEMORY_SIZE)

    eps = EPS_START

    rewards = []
    for ep in range(EPISODES):
        state = torch.tensor(get_state(best_v_s), dtype=torch.float32)

        if random.random() < eps:
            action = random.randint(0, num_actions - 1)
        else:
            with torch.no_grad():
                q_values = policy_net(state.unsqueeze(0).float()) # Ensure input is float32
                action = q_values.argmax().item()

        v_s_drl = np.full(s, action)  # simplify: same action for all
        l_total, e_total = compute_latency_energy(v_s_drl)
        reward = get_reward(l_total, e_total)
        next_state = torch.tensor(get_state(v_s_drl), dtype=torch.float32)  # approximate next

        memory.push(state.numpy(), action, next_state.numpy(), reward) # Store as numpy, convert later

        if len(memory) >= BATCH_SIZE:
            transitions = memory.sample(BATCH_SIZE)
            batch = Transition(*zip(*transitions))

            state_batch = torch.tensor(np.array(batch.state), dtype=torch.float32) # Convert to tensor with float32
            action_batch = torch.tensor(batch.action).unsqueeze(1)
            reward_batch = torch.tensor(batch.reward, dtype=torch.float32) # Convert to tensor with float32
            next_state_batch = torch.tensor(np.array(batch.next_state), dtype=torch.float32) # Convert to tensor with float32

            q_values = policy_net(state_batch).gather(1, action_batch).squeeze()

            next_q = target_net(next_state_batch).max(1)[0]
            expected = reward_batch + GAMMA * next_q

            loss = nn.MSELoss()(q_values, expected)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if ep % TARGET_UPDATE == 0:
            target_net.load_state_dict(policy_net.state_dict())

        eps = max(EPS_END, eps * EPS_DECAY)
        rewards.append(reward)

        print(f"Episode {ep}: Action {action}, L_total {l_total}, E_total {e_total}, Reward {reward}")

    avg_reward = np.mean(rewards)
    print(f"Average Reward: {avg_reward}")
    return policy_net

# Run simulation
best_v_s = age_moea()
drl_net = rainbow_drl(best_v_s)